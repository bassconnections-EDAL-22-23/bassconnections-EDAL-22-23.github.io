<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Bass Connections 2022-2023</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  cons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: eNno
  * Updated: Mar 10 2023 with Bootstrap v5.2.3
  * Template URL: https://bootstrapmade.com/enno-free-simple-bootstrap-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center justify-content-between">

      <!-- <h1 class="logo"><a href="index.html">Bass Connections 2022-2023</a></h1> -->
      <!-- Uncomment below if you prefer to use an image logo -->
      <!-- <a href="index.html" class="logo"><img src="assets/img/logo.png" alt="" class="img-fluid"></a> -->

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#motivation">Motivation</a></li>
          <li><a class="nav-link scrollto" href="#contributions">Contributions</a></li>
          <li><a class="nav-link scrollto" href="#ssl">Self-supervised learning</a></li>
          <li><a class="nav-link scrollto" href="#geonet">GeoNet dataset</a></li>
          <li class="dropdown"><a href="#experiments"><span>Experiments</span> <i class="bi bi-chevron-down"></i></a>
            <ul>
              <li><a href="#experiment_design">Experiment design</a></li>
              <li><a href="#benchmarking">Benchmarking</a></li>
            </ul>
          </li>
          <li><a class="nav-link scrollto" href="#results">Results</a></li>
          <li><a class="nav-link scrollto" href="#takeaways">Key Takeaways</a></li>
          <li><a class="nav-link scrollto" href="#future">Future steps</a></li>
          <li><a class="nav-link scrollto" href="#team">Our Team</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero" class="d-flex align-items-center">
    <!-- <img src="assets/img/earth-satellite.jpg"/> -->

    <div class="container">
      <div class="row">
        <div class="col-lg-12 pt-5 pt-lg-0 order-2 order-lg-1 d-flex flex-column align-items-center">
          <h1>Bass Connections 2022-2023</h1>
          <h2>Tracking Climate Change With Satellites and Artificial Intelligence</h2>
          <h3>Margaret Brooks, Francesca Chiappetta, Alex Desbans, Neel Gajjar, Julia Kourelakos, Saad Lahrichi, Vaishvi
            Patel, Ruixin Zhang, Ruohan Zhang, Shufan Xia</h3>
          <div class="d-flex">
            <a href="https://github.com/energydatalab/BassConnections22-23" class="btn-get-started scrollto">Our
              Github</a>
          </div>
        </div>
      </div>
    </div>

  </section><!-- End Hero -->

  <main id="main">
    <!-- ======= Motivation Section ======= -->
    <section id="motivation" class="about">
      <div class="container">

        <div class="row">

          <div class="section-title">
            <span>Motivation</span>
            <h2>Motivation</h2>
          </div>

          <div class="col-lg-6">
            <img src="assets/img/indusriver-before.jpg" class="img-fluid" alt="">
            <img src="assets/img/indusriver-after.jpg" class="img-fluid" alt="">
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <!-- <ul>
              <li><i class="bi bi-check-circle"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat</li>
              <li><i class="bi bi-check-circle"></i> Duis aute irure dolor in reprehenderit in voluptate velit</li>
              <li><i class="bi bi-check-circle"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate trideta storacalaperda</li>
            </ul> -->
            <p>
              Geospatial data has become increasingly important across disciplines as the availability of remote sensing
              data, including satellite imagery, proliferates. Large, continuous streams of data are publicly available
              from satellites such as Sentinel and Landsat. Traditionally, this data must be labeled to be used for
              machine learning models. However, labeling large datasets is expensive, requiring a lot of time and labor.
              Many large and labeled data sets are not diverse in geographic location and so cannot be generalized to
              the world. Currently, using a model pre-trained on imageNet can mitigate the need for large amounts of
              labeled data. But this method has limited application to remote sensing tasks due to the unique
              characteristics of satellite images. Recent research has shown that self-supervised learning (SSL)
              techniques have the potential to develop robust feature representations of geospatial imagery data with
              minimal task-specific labeled data, including Caron et al. (2020), Chen et al. (2020), and Calhoun et al.
              (2022). We present GeoNet, a dataset that exceeds the scale and diversity of any prior datasets, and
              evaluate its usefulness by comparing the performance of SSL models pre-trained using three different
              methods: SSL pretraining on ImageNet, SSL pretraining on GeoNet, and SSL pretraining on ImageNet further
              trained on GeoNet.
            </p>
          </div>
        </div>

      </div>
    </section><!-- End Motivation Section -->

    <!-- ======= Key Contributions Section ======= -->
    <section id="contributions" class="featured-services">
      <div class="container content">
        <div class="row">
          <div class="section-title">
            <span>Key Contributions</span>
            <h2>Key Contributions</h2>
          </div>
          <div class="col-lg-4 col-md-6">
            <div class="icon-box">
              <div class="icon"><i class="bi bi-database"></i></div>
              <h4 class="title"><a href="#contributions">Develop GeoNet</a></h4>
              <p class="description">GeoNet is the largest and most diverse dataset to be curated for SSL in remote
                sensing. It contains over 10 million images across geographies, cities, rural regions, biomes, areas
                impacted by global change, across 8 half-seasons — the first and largest ever to capture geospatial,
                temporal, and semantic diversity for remote sensing data.</p>
            </div>
          </div>
          <div class="col-lg-4 col-md-6 mt-4 mt-md-0">
            <div class="icon-box">
              <div class="icon"><i class="bi bi-graph-up"></i></div>
              <h4 class="title"><a href="#contributions">Test different training paradigms of SwAV on GeoNet</a></h4>
              <p class="description">We test different training paradigms of SwAV on GeoNet to find that GeoNet
                outperforms state-of-the-art models in three out of five downstream recognition tasks and is
                competitive with other SSL models </p>
            </div>
          </div>
          <div class="col-lg-4 col-md-6 mt-4 mt-lg-0">
            <div class="icon-box">
              <div class="icon"><i class="bi bi-gear"></i></div>
              <h4 class="title"><a href="#contributions">Publish GeoEye</a></h4>
              <p class="description">GeoEye, a large pre-trained encoder for RGB imagery, significantly reduces the
                quantity of labeled data required for downstream recognition tasks, making recognition methods far more
                accessible and compute-efficient to researchers</p>
            </div>
          </div>
        </div>

      </div>
    </section>
    <!-- End Key Contributions Section -->
    <!-- ======= Key Components of our work ======= -->
    <section id="mainstages" class="class=" about">
      <div class="container">
        <div class="row">
          <div class="section-title">
            <h2 style="text-align:center">
              Our work follows the below 5 stages:
            </h2>
          </div>
          <div class="col-lg-12 col-md-12">
            <div class="text-center">
              <img src="assets/img/5-stages.png" class="img-fluid" alt="" width="80%" height="auto">
            </div>
          </div>
        </div>

      </div>

    </section>
    <!-- End Key Components Section -->
    <!-- ======= SSL Section ======= -->
    <section id="ssl" class="services section-bg">
      <div class="container">
        <div class="row">
          <div class="section-title">
            <span><a href="#ssl"></a>Self-supervised learning (SSL)</span>
            <h2>Self-supervised learning (SSL)</h2>
          </div>
          <div class="col-lg-12 col-md-12">
            <div class="text-center">
              <img src="assets/img/ssVSssl.png" class="img-fluid half-screen" alt="">
            </div>
            <p style="text-align:center">
              Historical uses of remote sensing imagery for machine learning have primarily involved supervised
              learning: the use of labeled
              input data to train a model to perform classification tasks. This requires large, labeled remote sensing
              imagery datasets. Manually
              labeling these datasets is extremely time-consuming, requires expert knowledge, and must be repeated for
              each new dataset. (ImageNet,
              one of the largest and most widely used natural image recognition datasets with 14 million training
              images, required years of human
              labeling!) Furthermore, existing datasets of this nature are limited in geographic diversity,
              representing mainly Europe and America.
              Training machine learning models on data that covers only the Global North limits their transferability
              to other parts of the world.
            </p>
            <p style="text-align:center">
              Self-supervised learning (SSL) methods, which train a model to perform classification tasks without the
              need for labeled input data,
              address these limitations and remove the need for large, task-specific labeled datasets. Through
              pre-training on large, diverse,
              unlabeled datasets, self-supervised models learn to extract robust, highly transferable features that
              can then be applied to a wide
              range of other tasks and domains. This is highly useful for remote sensing applications.
            </p>
            <p style="text-align:center">
              One popular SSL technique is contrastive learning, in which a model is trained to discriminate between
              positive examples (pairs of
              data that are similar in some way) and negative examples (pairs of data that are dissimilar). Many
              self-supervised learning models
              trained using contrastive learning have been shown to close the gap with and outperform supervised
              learning models. One popular SSL
              model trained using contrastive learning is SwAV (Swapping Assignments between Views), which eliminates
              the need for pairwise
              comparisons by using clustering-based methods that compare multiple views of the same image.
            </p>
            <p style="text-align:center">
              However, commonly used pretraining datasets for self-supervised image classification and object
              detection tasks, such as ImageNet,
              have limited application to remote sensing tasks because of satellite images’ unique characteristics.
              (Images taken by satellites
              capture observations from above, come in multi-spectral bands instead of RGB, and do not include a
              unique element). We are aware of
              no equivalent to ImageNet that is specifically intended for pre-training for use with satellite imagery
              data.
            </p>
          </div>
        </div>
      </div>
    </section>
    <!-- End SSL Section ======= -->



    <!-- ======= GeoNet and sampling Section ======= -->
    <section id="geonet" class="about">
      <div class="container">
        <div class="row">
          <div class="section-title">
            <span>GeoNet Dataset</span>
            <h2 href="#geonet">GeoNet Dataset</h2>
          </div>
          <div class="col-lg-6">
            <img src="assets/img/sample-distribution.png" class="img-fluid" alt="">
            <caption>
              The graph shows how we drew samples from different locations covering the entire globe except
              Antarctica, demonstrating the geospatial diversity of GeoNet.
            </caption>
            <div class="spacer"> </div>
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <p>
              To create an immense dataset that captures geospatial, temporal, and semantic diversity, we select
              features that cover diversity across geographies (all continents and countries), cities (all major cities are
              included), rural regions (across all continents and countries), biomes (forests, deserts, tundra, etc.), areas
              impacted by global change (floods, fires, storms, etc.), across 8 half-seasons.
            </p>
            <p>
              Data was collected from 7.2M unique 2.24km x 2.24km areas on all continents except Antarctica. Urban and
              rural areas comprise 60% of the 10 million images, 19% for natural disaster areas, 20% for land use and
              cover, and 1% for specific built environment features. Furthermore, GeoNet is temporally diverse as it involved extracting
              Sentinel-2 images from different time periods. 
            </p>
          </div>
          <div class="row"> 
            <p class="col-lg-12">
              We used 4 sampling methods to build this dataset to have diversity across urban vs rural regions, across built vs natural environments, across different land covers, and across natural disaster prone regions. 
            </p>
          </div>
          <div class="row">
              <h4>Method 1: Urban vs Rural</h4>
              <img src="assets/img/method1_urban.png" class="img-fluid col-6" alt="">
              <img src="assets/img/method1_rural.png" class="img-fluid col-6" alt="">
              <caption>The first method consists of urban sampling and rural sampling. For urban sampling, we first draw samples randomly from a Multivariate normal (MVN) distribution centered at city centers worldwide with probability weighted by log (population). We then draw a random sample coordinate from the corresponding MVN distribution located within a 50 km radius. For rural sampling, we randomly sample locations with population densities between 5 and 250 people per km^2 in a 30 X 30 arcsec (1 X 1 km at the equator approximately) resolution population density map.</caption>
              <div class="spacer"></div>
          </div>
          <div class="row">
            <h4>Method 2: Built vs Natural Environment</h4>
            <img src="assets/img/method2_built.png" class="img-fluid half-screen" alt="">
            <caption>The second method includes Seven categories of human-built features associated with climate change globally are identified using various datasets resulting in around 82,000 coordinates. all the coordinates of major human-built features that are associated with climate change, including airports, mining sites, power plants, ports, oil rigs, wind turbines, and dams. </caption>
            <div class="spacer"></div>
          </div>
          <div class="row">
            <h4>Method 3: Land Type</h4>
            <img src="assets/img/method3_landtype.png" class="img-fluid half-screen" alt="">
            <caption>The third method uses a stratified sampling approach to include a fixed amount of images are included for 11 land cover types: Artificial land, Cropland, Grassland, Tree-covered areas, Shrubs-covered areas, Herbaceous vegetation that is aquatic or regularly flooded, Mangroves, Sparse vegetation, Bare soil, Snow and glaciers, Water bodies.
            </caption>
            <div class="spacer"></div>
          </div>
          <div class="row">
            <h4>Method 4: Natural Disasters</h4>
            <img src="assets/img/method4_natural_disaster.png" class="img-fluid half-screen" alt="">
            <caption>The fourth and final method samples locations vulnerable to climate change-related natural disasters. For each of the four categories of natural disaster (wildfires, cyclones, droughts, and floods), we use a “proxy variable” as the probability in a weighted random sampling, which either quantifies the severity of the disaster or its occurrence frequency.</caption>
            <div class="spacer"></div>
          </div>
        </div>

        <div class="row">
          <h5> Sampling Strategy with a Grid-Based Approach to avoid duplications and overlaps in extracted images </h5>
          <p> A 2.24x2.24 km grid is defined to match the image size for SwAV's
            input dimensions (224x224) and the resolution of Sentinel-2 images (10 m). Mapping coordinates into a
            discrete grid allows for the removal of nearby locations with overlapping scenes. Coordinated used to extract Sentinel-2 images are collected with various
            considerations:
          </p>
          <ol>
            <li>
              10 million samples are generated using all four sampling methods after
              which the corresponding grid cells that contain these coordinates are determined. Images from
              oversampled grid cells are deleted by limited the number of images per grid cell to 4.
            </li>
            <li>
              If a grid cell is sampled more than once, its corresponding images from
              different search periods are extracted enabling seasonal temporal diversity and reducing redundancy of
              the view.
            </li>
          </ol>
          </div>
        </div>

      </div>
    </section><!-- End Methods and Materials Section -->

    <!-- ======= Benchmark Dataset Section ======= 
    <section id="dataset" class="about">
      <div class="container">

        <div class="row">
          <div class="section-title">
            <span>b</span>
            <h2>B</h2>
          </div>
          <div class="col-lg-6">
            <img src="assets/img/about.png" class="img-fluid" alt="">
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <p class="fst-italic">
              Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore
              magna aliqua.
            </p>
            <ul>
              <li><i class="bi bi-check-circle"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat</li>
              <li><i class="bi bi-check-circle"></i> Duis aute irure dolor in reprehenderit in voluptate velit</li>
              <li><i class="bi bi-check-circle"></i> Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate trideta storacalaperda</li>
            </ul>
            <p>
              Ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate
              velit esse cillum dolore eu fugiat nulla pariatur.
            </p>
          </div>
        </div>

      </div>
    </section> End Benchmark Dataset Section -->

    <!-- ======= Experiments Section ======= -->
    <section id="experiments" class="services section-bg">
      <div class="container">

        <div class="section-title">
          <span>Experiments</span>
          <h2>Experiments</h2>
        </div>

        <div class="row">
          <div id="experiment_design" class="col-lg-12 col-md-12">
            <div class="icon-box">
              <!-- <div class="icon"><i class="bx bx-world"></i></div> -->
              <h4><a href="#experiment_design">Experiment Design</a></h4>
              <p>
                We aim to compare the impact of different pretraining paradigms on the transfer learning performance of
                self-supervised learning
                models and fully supervised learning models. We experiment five different pretraining methods, including
                pretraining on ImageNet
                and pretraining on remote sensing datasets, as shown in the table.
              </p>
              <br />
              <img src="assets/img/experiments.png" class="img-fluid" alt="" width="55%">
              <br />
              <p>
                For each pretraining method, we apply and evaluate its transfer learning performance on five downstream
                benchmark inference tasks
                in table. To this end, we developed a pipeline for applying pre-training models.
              </p>
              <br />
              <img src="assets/img/pipeline.png" class="img-fluid" alt="" width="75%">
              <br />
              <p>
                As shown in the figure, the experiment mainly consists of two steps. First, we use the ResNet50 backbone
                from each pretrained encoder
                to extract representations. Second, we attach a decoder to the ResNet50 backbone, either Unet for
                segmentation tasks or a fully
                connected linear layer for classification tasks, and fine-tune the decoder using the labels provided by
                the benchmark dataset.
              </p>
              <br />
              <img src="assets/img/metrics.png" class="img-fluid" alt="" width="50%">
            </div>
          </div>

        </div>
        <div id="benchmarking" class="col-lg-12 col-md-12">
          <div class="icon-box">
            <!-- <div class="icon"><i class="bx bx-tachometer"></i></div> -->
            <h4><a href="#benchmarking">Benchmarking</a></h4>
            <p>
              We assemble a list of accessible, well-studied, and highly representative benchmark datasets for two
              categories of downstream tasks:
              classification and semantic segmentation. From the list, we prioritized Sentinel-2 images with a wider
              geographical coverage and
              restricted the image resolution between 0.1 and 30 m/pixel. Preprocessing is conducted so that our
              benchmark images to all be of size
              224 x 224 pixels, aligning with the image size of the GeoEye dataset used for pretraining our model. We
              evaluate GeoNet on these datasets
              by comparing its performance to the performance of fully-supervised models and other state-of-the-art
              SSL models.
            </p>
            <br />
            <img src="assets/img/benchmark_datasets.png" class="img-fluid" alt="" width="90%">
          </div>
        </div>



      </div>
    </section>
    <!-- End Experiments Section -->

    <!-- ======= Results Section ======= -->
    <section id="results" class="about">
      <div class="container">

        <div class="row">
          <div class="section-title">
            <span>Results</span>
            <h2>Results</h2>
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <p>
              We identified which methods perform better across training size. SwAV-3M appears to outperform other
              pre-training methods
              for the SEN12MS dataset, which has weak labels. For Big Earth Net, both SwAV 3M and SwAV Imagenet perform
              well in multilabel
              classification tasks, with SwAV 3M performing the best with a larger training size. For Deep Globe,
              SwAV-3M, SwAV Imagenet,
              and supervised Imagenet all performed well, with supervised imagenet performing best for a lower training
              size and SwAV imagenet
              performing best for a larger training size. SeCo performed noticeably worse, and MoCoV2_fMoW produced by
              far the lowest IOU results.
              For Sustain Bench, SwAV 3M and SwAV Imagenet outperform other pre-training methods when the training size
              is small, while
              increasing the training size, all models except SeCo perform similarly. For EuroSat, SwAV image net
              performs the best, and
              unfortunately SwAV 3M does not perform better than supervised.
            </p>
            <p>
              The results of our experiments are summarized below. For each benchmark task, we plot the accuracy
              measurement at different training
              size used in fine-tuning, showing each pretraining method with each separate line.
            </p>
          </div>
          <div class="col-lg-6">
            <img src="assets/img/results_table.png" class="img-fluid" alt="">
          </div>
          <div class="spacer"></div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <h4>SEN12MS</h4>
            <div class="row">
              <div class="col-lg-6">
                <img src="assets/img/SEN12MS_accuracy_allsize.png" class="img-fluid" alt="">
              </div>
              <div class="col-lg-6">
                <img src="assets/img/SEN12MS_loss_allsize.png" class="img-fluid" alt="">
              </div>
            </div>
            <p>
              SwAV-3M consistently outperformed SeCo, mocoV2 trained on fMoW, supervised trained on imagenet, and SwAV
              trained on image net on the
              SEN12MS dataset. SEN12MS is a weakly-labeled semantic segmentation dataset, therefore all the pre-training
              methods are limited,
              hence the low IoU values. The IoU score for pre-training methods increases significantly as training size
              increases from 64 to 1024
              images. The results also show that self-supervision can outperform supervised on this weak label
              segmentation task, except for SeCo.
            </p>
            <div class="spacer"></div>
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <h4>BigEarthNet</h4>
            <div class="row">
              <div class="col-lg-6">
                <img src="assets/img/bigearthnet_accuracy.png" class="img-fluid" alt="">
              </div>
              <div class="col-lg-6">
                <img src="assets/img/bigearthnet_loss.png" class="img-fluid" alt="">
              </div>
            </div>
            <p>
              SwAV-3M, SwAV trained on ImageNet, and mocoV2 trained on fMoW outperformed the other 2 models, for this
              multilabel classification task,
              at a top-1 precision score between 0.65 and 0.7. SwAV-3M performed slightly better than SwAV trained on
              ImageNet and mocoV2 trained on
              fMoW for smaller training sizes (64, 256, and 512). The improvement in performance gradually decreases
              with larger training sizes across
              the 5 different approaches.
            </p>
            <div class="spacer"></div>
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <h4>SustainBench</h4>
            <div class="row">
              <div class="col-lg-6">
                <img src="assets/img/field_delineation_accuracy_allsize.png" class="img-fluid" alt="">
              </div>
              <div class="col-lg-6">
                <img src="assets/img/field_delineation_loss_allsize.png" class="img-fluid" alt="">
              </div>
            </div>
            <p>
              Swav 3M and swav imageNet outperform other pretraining methods when the training size is small (64, 128,
              256 and 512). However as the
              training size increases to 1024, the marginal benefit from swav pretraining compared to other methods
              decreases. Pretraining SWAV on
              3M sentinel-2 images has a small improvement from Imagnet pretraining. Overall, the best IoU score for
              pretraing with SWaV on both our
              dataset and imagenet are comparable to those in Calhoun et al. (2022) at around 0.44.
            </p>
            <div class="spacer"></div>
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <h4>EuroSat</h4>
            <div class="row">
              <div class="col-lg-6">
                <img src="assets/img/euroSat_accuracy_allsize.png" class="img-fluid" alt="">
              </div>
              <div class="col-lg-6">
                <img src="assets/img/euroSat_loss_allsize.png" class="img-fluid" alt="">
              </div>
            </div>
            <p>
              MoCoV2, swav_imagenet, and supervised_imagenet all performed better than SwAV-3M. SeCo performed
              noticeably worse than the other tasks.
            </p>
            <div class="spacer"></div>
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <h4>DeepGlobe</h4>
            <div class="row">
              <div class="col-lg-6">
                <img src="assets/img/deep_globe_accuracy_allsize.png" class="img-fluid" alt="">
              </div>
              <div class="col-lg-6">
                <img src="assets/img/deep_globe_loss_allsize.png" class="img-fluid" alt="">
              </div>
            </div>
            <p>
              Swav_3M, swav_imagenet, and supervised_imagenet all performed the best with IOU values comparable to those
              in the Deep Globe paper. SeCo
              performed noticeably worse and MoCoV2_fMoW produced by far the lowest IOU results. Multiple reruns of
              MoCoV2 did not result in any
              improvements in IOU score.
            </p>
            <div class="spacer"></div>
          </div>
        </div>

      </div>
    </section><!-- End Results Section -->

    <!-- ======= Key Takeaways Section ======= -->
    <section id="takeaways" class="featured-services">
      <div class="container content">
        <div class="row">
          <div class="section-title">
            <span>Key Takeaways</span>
            <h2>Key Takeaways</h2>
          </div>
          <div class="col-lg-4 col-md-6">
            <div class="icon-box">
              <div class="icon"><i class="bi bi-1-square"></i></div>
              <h5 class="title">SSL models dominantly out-perform (except for SeCO) in different types of downstream tasks for the label-limited scenario.</h5>
            </div>
          </div>
          <div class="col-lg-4 col-md-6 mt-4 mt-md-0">
            <div class="icon-box">
              <div class="icon"><i class="bi bi-2-square"></i></div>
              <h5 class="title">SwAV pretraining is winning compared to other SSL methods. SwAV-3M is competitive, if not winning, on some of these datasets such as field delineation and SEN12MS.</h5>
            </div>
          </div>
          <div class="col-lg-4 col-md-6 mt-4 mt-lg-0">
            <div class="icon-box">
              <div class="icon"><i class="bi bi-3-square"></i></div>
              <!-- <h4 class="title"><a href="">Sed ut perspiciatis</a></h4> -->
              <h5 class="title">SwAV3M does not outperform swav-imagenet could due to domain transferring from imagenet to satellite images have not been completed. More investigations are needed.</h5>
            </div>
          </div>
        </div>

      </div>
    </section><!-- End Key Rakeaways Section -->

    <!-- ======= Future Steps Section ======= -->
    <section id="future" class="about">
      <div class="container">

        <div class="row">
          <div class="section-title">
            <span>Future Steps</span>
            <h2>Future Steps</h2>
          </div>
          <div class="col-lg-6">
            <img src="assets/img/future.png" class="img-fluid" alt="">
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 content">
            <p>
              Future steps in further evaluating the performance of our encoder on GeoNet could include the use of more
              benchmark datasets, expansion to higher resolution and addition of weak labels. Testing our encoder
              against benchmark datasets will more extensively show the diverse potential of its applications. Currently
              our dataset consists of images of resolution 10 m per pixel. We can further expand our testing on
              benchmark tasks which are higher in resolution and also expand the dataset to be higher resolution to see
              how our GeoNet self-supervised encoder performs on those tasks.
            </p>
            <p>
              Our second hypothesis is that a natural language processing (NLP) encoder enables mappings between textual
              annotation and satellite images, allowing customized queries of climate data. The textual data has already
              been gathered and the self supervised encoder has been trained. As our next step, we want to create a
              natural language processing model to extract text features from the text data collected so we can support
              a variety of questions. Then combine those text features from the natural language encoder and the image
              features from our self supervised encoder from before training using the geographic data and related weak
              labels to both. After this combination the previously unlabeled images will now have text features. This
              could be presented using a basic web application that allows text searches from users and presents the
              related images!!

            </p>
          </div>
        </div>

      </div>
    </section><!-- End Future Steps Section -->

    <!-- ======= Team Section ======= -->
    <section id="team" class="team section-bg">
      <div class="container">

        <div class="section-title">
          <span>Our Team</span>
          <h2>Our Team</h2>
        </div>

        <div class="row">
          <div class="content">
            <h3>Undergraduate Team Members</h3>
          </div>
          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/maggie.jpg" alt="">
              <h4>Margaret Brooks</h4>
              <p>
                Major
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href=""><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/alex.png" alt="">
              <h4>Alex Desbans</h4>
              <p>
                Major
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href=""><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/neel.jpeg" alt="">
              <h4>Neel Gajjar</h4>
              <p>
                Major
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href=""><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/jules.png" alt="">
              <h4>Julia Kourelakos</h4>
              <p>
                Computer Science
              </p>
              <div class="social">
                <a href="https://github.com/juliakourela"><i class="bi bi-github"></i></a>
                <a href="https://www.linkedin.com/in/jules-kourelakos-505476206/"><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/saad.jpg" alt="">
              <h4>Saad Lahrichi</h4>
              <p>
                Major
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href=""><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/vaishvi.jpg" alt="">
              <h4>Vaishvi Patel</h4>
              <p>
                Electrical & Computer Engineering, Computer Science
              </p>
              <div class="social">
                <a href="https://github.com/vaishvi-patel"><i class="bi bi-github"></i></a>
                <a href="https://www.linkedin.com/in/vaishvipatel/"><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/edna.png" alt="">
              <h4>Ruixin Zhang</h4>
              <p>
                Neuroscience & Computer Science, Minor in Cinematic Arts
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href=""><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/ada.JPG" alt="">
              <h4>Ruohan Zhang</h4>
              <p>
                Computer Science & Mathematics
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href="https://www.linkedin.com/in/ruohan-ada-zhang/"><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>
        </div>

        <div class="row">
          <div class="content">
            <h3>Graduate Project Managers</h3>
          </div>
          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/frankie.png" alt="">
              <h4>Francesca Chiappetta</h4>
              <p>
                Graduate Program
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href=""><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/shufan.png" alt="">
              <h4>Shufan Xia</h4>
              <p>
                Graduate Program
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href=""><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>
        </div>

        <div class="row">
          <div class="content">
            <h3>Faculty Team Leaders</h3>
          </div>
          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/kyle.png" alt="">
              <h4>Kyle Bradbury</h4>
              <span>Pratt School of Engineering</span>
              <p>
                Electrical & Computer Engineering, Nicholas Institute for Energy, Environment, and Sustainability
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href=""><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member">
              <img src="assets/img/team/jordan.png" alt="">
              <h4>Jordan Malof</h4>
              <span>Pratt School of Engineering</span>
              <p>
                Electrical & Computer Engineering, Energy Initiative
              </p>
              <div class="social">
                <a href=""><i class="bi bi-github"></i></a>
                <a href=""><i class="bi bi-linkedin"></i></a>
              </div>
            </div>
          </div>
        </div>

      </div>
    </section>
  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">

    <div class="footer-top">

      <div class="container">

        <div class="row justify-content-center">
          <div class="col-lg-6">
            <h3>Bass Connections 2022-2023</h3>
            <p>Tracking Climate Change With Satellites and Artificial Intelligence</p>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-lg-12 pt-5 pt-lg-0 order-2 order-lg-1 d-flex flex-column align-items-center">
            <div class="d-flex">
              <a href="https://github.com/energydatalab/BassConnections22-23" class="btn-get-started"> Github <i
                  class="bi bi-github"></i> </a>
            </div>
          </div>
        </div>


      </div>
    </div>

    <div class="container footer-bottom clearfix">
      <div class="copyright">
        &copy; Copyright <strong><span>eNno</span></strong>. All Rights Reserved
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/enno-free-simple-bootstrap-template/ -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i
      class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>